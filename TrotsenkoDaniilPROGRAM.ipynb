{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hashlib\n",
    "import warnings\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, LabelEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data = pd.read_csv(\"tabular_data.csv\", sep=',')\n",
    "hashed_feature = pd.read_csv(\"hashed_feature.csv\", sep=',')\n",
    "train = pd.read_csv(\"train.csv\", sep=',')\n",
    "test = pd.read_csv(\"test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "\n",
    "def replace_df(df):\n",
    "    x = df.copy()\n",
    "    x[df.columns] = full_pipeline.fit_transform(df)[:, :]\n",
    "    return x\n",
    "\n",
    "\n",
    "def OneHotEncoder_features(x, names):\n",
    "    lb_style : LabelBinarizer = LabelBinarizer()\n",
    "    lb_results = lb_style.fit_transform(x[names])\n",
    "    new_tabular_data =pd.concat( [x[['id']],\n",
    "        pd.DataFrame(lb_results, columns=[f\"{names}_{i}\" for i in range(len(lb_style.classes_))])\n",
    "    ], axis=1)\n",
    "    \n",
    "    return new_tabular_data.groupby(by=['id']).sum()\n",
    "\n",
    "\n",
    "def OneHotEncoder_for_Hashed_data(data):\n",
    "    x = np.array(data['feature_50'].value_counts()[:3].index)\n",
    "    data.fillna(x[0], inplace=True)\n",
    "    data['feature_50'] = data['feature_50'].astype('category')\n",
    "    \n",
    "    new_hashed_feature = data.groupby(by=['id']).count()\n",
    "    new_hashed_feature['id'] = new_hashed_feature.index\n",
    "    new_hashed_feature.index=pd.RangeIndex(0, new_hashed_feature.shape[0], 1)\n",
    "    \n",
    "    return new_hashed_feature\n",
    "    \n",
    "    \n",
    "def union_with_target(df1, df2, df3, df4):\n",
    "    df1_s = df1[[f'feature_{i}' for i in (list(range(0,25)) + list(range(26, 50)))]+['id']].groupby(by=['id']).sum()\n",
    "    df2_s = df2.groupby(by=['id']).sum()\n",
    "    \n",
    "    df_2_s = pd.merge(df2_s, df3, how='inner', on='id')\n",
    "    return pd.merge(pd.merge(df1_s, df_2_s, how='inner', on='id'), df4, how='inner', on='id')\n",
    "\n",
    "def union_with_test(df1, df2, df3):\n",
    "    df1_s = df1[[f'feature_{i}' for i in (list(range(0,25)) + list(range(26, 50)))]+['id']].groupby(by=['id']).sum()\n",
    "    df2_s = df2.groupby(by=['id']).sum()\n",
    "    \n",
    "    df_2_s = pd.merge(df2_s, df3, how='inner', on='id')\n",
    "    return pd.merge(df1_s, df_2_s, how='inner', on='id')\n",
    "    \n",
    "    \n",
    "class DataFrameSelector (BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "class DataFrameCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X) -> pd.DataFrame:\n",
    "        tmp = self.df.copy()\n",
    "        tmp[[f'feature_{i}' for i in (list(range(0,25)) + list(range(26, 50)))]] = X[:, :]\n",
    "        return tmp\n",
    "\n",
    "feature_pipeline = Pipeline([\n",
    "    (\"Selector\", DataFrameSelector([f'feature_{i}' for i in (list(range(0,25)) + list(range(26, 50))) ])),\n",
    "    (\"Imputer\", SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('Scaler', StandardScaler()),\n",
    "    (\"CreatorDF\", DataFrameCreator(tabular_data)),\n",
    "])\n",
    "\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"feature\", feature_pipeline),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = union_with_target(\n",
    "                        replace_df(tabular_data),\n",
    "                        OneHotEncoder_features(tabular_data, \"feature_25\"),\n",
    "                        OneHotEncoder_for_Hashed_data(hashed_feature),\n",
    "                        train\n",
    ")\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(full_data[[f'feature_25_{i}' for i in range(0,8)]+[\"feature_50\"]])\n",
    "\n",
    "full_data[[f'feature_25_{i}' for i in range(0,8)]+[\"feature_50\"]] = sc.fit_transform(full_data[[f'feature_25_{i}' for i in range(0,8)]+[\"feature_50\"]])[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = full_data[full_data.columns[1:-1]], full_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc_score(model, X_valid, y_valid):\n",
    "    return roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Датасет\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y, train_size=.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7484509001004405"
      ]
     },
     "execution_count": 760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra = ExtraTreesClassifier(random_state=0, n_jobs=-1, n_estimators=800, max_leaf_nodes=150)\n",
    "\n",
    "extra.fit(X_train, y_train)\n",
    "get_roc_auc_score(extra, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7382948311828788"
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier( max_depth=2 ) , n_estimators=250,\n",
    "algorithm= \"SAMME.R\" , learning_rate=0.01)\n",
    "ada_clf.fit( X_train , y_train )\n",
    "\n",
    "get_roc_auc_score(ada_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pca_kernel = Pipeline(\n",
    "    [(\"kpca\", KernelPCA(n_components=10)),\n",
    "    (\"log_reg\", LogisticRegression())]\n",
    ")\n",
    "param_grid = [\n",
    "    {\n",
    "        \"kpca__gamma\":np.linspace(0.03, 0.05, 10),\n",
    "        \"kpca__kernel\":[\"rbf\", \"sigmoid\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "pca_k = GridSearchCV(clf_pca_kernel, param_grid, cv=3, n_jobs=-1)\n",
    "pca_k.fit(X_train, y_train)\n",
    "roc_auc_score(y_test,pca_k.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "get_roc_auc_score(final_model, X_test, y_test)\n",
    "roc_auc_score(y_test, grid_search.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_cl = DecisionTreeClassifier(random_state=0, max_depth=5, max_features=10)\n",
    "cross_val_score(tree_cl, X_train, y_train, cv=5).mean()\n",
    "y_pred = cross_val_predict(tree_cl, X_train, y_train, cv=5)\n",
    "\n",
    "confusion_matrix(y_train, y_pred)\n",
    "precision_score(y_train, y_pred), recall_score(y_train, y_pred)\n",
    "\n",
    "tree_cl.fit(X_train, y_train)\n",
    "get_roc_auc_score(tree_cl, X_test, y_test)\n",
    "roc_auc_score(y_test, tree_cl.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_first = KNeighborsClassifier(n_neighbors=20)\n",
    "cross_val_score(knn_first, X_train, y_train, cv=5).mean()\n",
    "\n",
    "\n",
    "y_pred = cross_val_predict(knn_first, X_train, y_train, cv=5)\n",
    "\n",
    "confusion_matrix(y_train, y_pred)\n",
    "precision_score(y_train, y_pred), recall_score(y_train, y_pred)\n",
    "knn_first.fit(X_train, y_train)\n",
    "get_roc_auc_score(knn_first, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'n_estimators': [3,10,20, 30, 50, 100],\n",
    "        'max_features':[2, 4, 6, 8, 10, 12], \"max_depth\":[i for i in range(5, 20, 2)]\n",
    "    },\n",
    "    {\n",
    "        'bootstrap':[False], \"n_estimators\":[3,6,8, 10], \"max_features\":[2,3,4, 6, 8, 10], \"max_depth\":[i for i in range(5, 16, 2)]\n",
    "    }\n",
    "]\n",
    "\n",
    "#random_forest_cl = RandomForestClassifier(random_state=0, n_jobs=5, max_depth=15)\n",
    "#random_forest_cl.fit(X_train, y_train)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "random_forest_cl = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=500, max_leaf_nodes=16, criterion='entropy')\n",
    "#random_forest_cl.fit(X_train, y_train)\n",
    "#grid_search = GridSearchCV(random_forest_cl, param_grid, cv=5, scoring='roc_auc')\n",
    "#grid_search.fit(X_train, y_train)\n",
    "#final_model = grid_search.best_estimator_\n",
    "\n",
    "#results  = cross_val_score(random_forest_cl, X_train, y_train, cv=skf)\n",
    "#get_roc_auc_score(random_forest_cl, X_test, y_test)\n",
    "\n",
    "\n",
    "parameters = {'max_features': [4, 7, 10, 13], 'min_samples_leaf': [1, 3, 5, 7], 'max_depth': [5,10,15,20], \"criterion\":['gini', 'entropy']}\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42, \n",
    "                             n_jobs=-1, oob_score=True)\n",
    "gcv = GridSearchCV(rfc, parameters, n_jobs=-1, cv=skf, verbose=1)\n",
    "gcv.fit(X_train, y_train)\n",
    "\n",
    "get_roc_auc_score(gcv, X_test, y_test)\n",
    "roc_auc_score(y_test, gcv.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, gcv.predict(X_test))\n",
    "#gcv.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_cl.fit(X_train, y_train)\n",
    "#y_probas_forest = cross_val_predict(random_forest_cl, X_train, y_train, cv=3, method=\"predict_proba\")\n",
    "\n",
    "#confusion_matrix(y_train, y_pred)\n",
    "#precision_score(y_train, y_pred), recall_score(y_train, y_pred), f1_score(y_train, y_pred), get_roc_auc_score(final_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "lr = LogisticRegression(\n",
    "    multi_class=\"multinomial\", solver='lbfgs', C=.1,\n",
    "    random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "y_probas_log = cross_val_predict(lr, X_train, y_train, cv=3, method=\"predict_proba\")\n",
    "lr.score(X_test, y_test)\n",
    "get_roc_auc_score(lr, X_test, y_test)\n",
    "roc_auc_score(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5882059800664452"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM\n",
    "svm = LinearSVC(C=1.1, loss='hinge')\n",
    "svm.fit(X_train, y_train)\n",
    "y_probas_svm = cross_val_predict(svm, X_train, y_train, cv=3, method=\"decision_function\")\n",
    "svm.score(X_test, y_test)\n",
    "roc_auc_score(y_test, svm.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5599667774086379"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pol = SVC(kernel='poly', degree=3, coef0=5, C=.1, random_state=0)\n",
    "svm_pol.fit(X_train, y_train)\n",
    "y_probas_svm_poly = cross_val_predict(svm, X_train, y_train, cv=3, method=\"decision_function\")\n",
    "svm_pol.score(X_test, y_test)\n",
    "roc_auc_score(y_test, svm_pol.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6051495016611296"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(loss='hinge',random_state=0)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "y_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3, method=\"decision_function\")\n",
    "#confusion_matrix(y_train, y_pred)\n",
    "#precision_score(y_train, y_pred), recall_score(y_train, y_pred)\n",
    "sgd_clf.score(X_test, y_test)\n",
    "roc_auc_score(y_test, sgd_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5867109634551495"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"lr\", lr), (\"rf\", random_forest_cl), ('svc', svm)\n",
    "    ], voting='hard'#or soft if all models have predict_proba\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, voting_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5862126245847176"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=0), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=0\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, bag_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recision_recall_vs_treshold(name, precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], 'b--', label='Точность')\n",
    "    plt.plot(thresholds, recalls[:-1], 'g-', label='Полнота')\n",
    "    plt.xlabel(\"Порог\")\n",
    "    plt.legend(loc='center left')\n",
    "    plt.ylim([0,1])\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_roc_curve(fpr, tpr, label):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_recision_recall_vs_treshold(\"SGD\",*precision_recall_curve(y_train, y_pred))\n",
    "#plot_recision_recall_vs_treshold(\"RandomForest\", *precision_recall_curve(y_train, y_probas_forest[:,1]))\n",
    "#plot_recision_recall_vs_treshold(\"LogRegression\", *precision_recall_curve(y_train, y_probas_log[:,1]))\n",
    "#plot_recision_recall_vs_treshold(\"SVM\",*precision_recall_curve(y_train, y_probas_svm))\n",
    "#plot_recision_recall_vs_treshold(\"SVM\",*precision_recall_curve(y_train, y_probas_svm_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#plot_roc_curve(fpr, tpr, \"SGD\")\n",
    "\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_probas_forest[:,1])\n",
    "#plot_roc_curve(fpr, tpr,  \"RandomForest\")\n",
    "\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_probas_log[:,1])\n",
    "#plot_roc_curve(fpr, tpr,  \"LogRegression\")\n",
    "\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_probas_svm)\n",
    "#plot_roc_curve(fpr, tpr,  \"SVM\")\n",
    "\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_probas_svm_poly)\n",
    "#plot_roc_curve(fpr, tpr,  \"SVM POL\")\n",
    "\n",
    "#roc_auc_score(y_train, y_probas_log[:,1]), roc_auc_score(y_train, y_probas_forest[:,1]), roc_auc_score(y_train, y_probas_svm), roc_auc_score(y_train, y_probas_svm_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание полноценной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features_importances = random_forest_cl.feature_importances_\\nfeatures_ = sorted(list(zip(features_importances, X.columns)), key=lambda x: x[0], reverse=True)\\nnames_features = [i[1] for i in list(filter( lambda x: x[0] > 0.01 , features_))]\\nnames_features, len(names_features)\\nfeatures_\\n\\nX = full_data[names_features]\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.80, stratify=y)'"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"features_importances = random_forest_cl.feature_importances_\n",
    "features_ = sorted(list(zip(features_importances, X.columns)), key=lambda x: x[0], reverse=True)\n",
    "names_features = [i[1] for i in list(filter( lambda x: x[0] > 0.01 , features_))]\n",
    "names_features, len(names_features)\n",
    "features_\n",
    "\n",
    "X = full_data[names_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.80, stratify=y)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.660874604033068"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    GridSearch for Decision tree\n",
    "\"\"\"\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "pipe_tree = Pipeline( steps=[\n",
    "    (\"pca\", PCA()),\n",
    "    (\"dec_tree\", DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "n_components = list(range(1, X_train.shape[1]+1))\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12]\n",
    "\n",
    "parameters = dict(pca__n_components=n_components,\n",
    "                      dec_tree__criterion=criterion,\n",
    "                      dec_tree__max_depth=max_depth)\n",
    "clf_GS = GridSearchCV(pipe_tree, parameters, cv=5, n_jobs=-1)\n",
    "clf_GS.fit(X_train, y_train)\n",
    "\n",
    "model_decision_tree = clf_GS.best_estimator_\n",
    "roc_auc_score(y_test, model_decision_tree.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  84 out of  84 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6876805995518814"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    GridSearch for KNN\n",
    "\"\"\"\n",
    "grid_for_KNN = {\n",
    "    \"n_neighbors\":[3,4, 5,7, 11, 15, 19],\n",
    "    'weights':['uniform', 'distance'],\n",
    "    \"metric\":['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "gs_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    grid_for_KNN,\n",
    "    verbose=1,\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs_knn.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, gs_knn.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, penalty='l1', solver='liblinear')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7396816812176465"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Grid Search for Logistic Regression\n",
    "\"\"\"\n",
    "\n",
    "grid_values = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C':[0.001,.009,0.01,.09,.1,1,5,10,25, 100],\n",
    "    'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "grid_clf_acc = GridSearchCV(LogisticRegression(), param_grid = grid_values, scoring = 'roc_auc', n_jobs=-1)\n",
    "grid_clf_acc.fit(X_train, y_train)\n",
    "print(grid_clf_acc.best_estimator_)\n",
    "roc_auc_score(y_test, grid_clf_acc.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_features must be in (0, n_features]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-743-93f59c8e893f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgradient_boosting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgradient_boosting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_boosting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_depth must be greater than zero. \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_features must be in (0, n_features]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n",
      "\u001b[1;31mValueError\u001b[0m: max_features must be in (0, n_features]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    GridSearch for GradientBoosting\n",
    "\"\"\"\n",
    "\n",
    "gradient_boosting = GradientBoostingClassifier( random_state=0, n_estimators=300, learning_rate=0.01, max_features=30, max_depth=5)\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "\n",
    "roc_auc_score(y_test, gradient_boosting.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7497952561230009"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    AdaBooster with LGR\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "adb_lgr = AdaBoostClassifier(LogisticRegression(C=100, penalty='l1', solver='liblinear'),random_state=0, n_estimators=3)\n",
    "adb_lgr.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, adb_lgr.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7350312425848294"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    GridSearch for RandomForest\n",
    "\"\"\"\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "random_forest_best = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = skf, n_jobs = -1, verbose = 2)\n",
    "random_forest_best.fit(X_train, y_train)\n",
    "\n",
    "roc_auc_score(y_test, random_forest_best.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances = random_forest_best.best_estimator_.feature_importances_\n",
    "features_ = list(zip(features_importances, X.columns))\n",
    "features_sorted = sorted(features_, key=lambda x: x[0], reverse=True)\n",
    "names_features = [i[1] for i in list(filter( lambda x: x[0] > 0.02 , features_))]\n",
    "names_features, len(names_features)\n",
    "\n",
    "X = full_data[names_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.80, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.15569512\n",
      "Iteration 2, loss = 0.66425972\n",
      "Iteration 3, loss = 0.56434910\n",
      "Iteration 4, loss = 0.56183127\n",
      "Iteration 5, loss = 0.53575209\n",
      "Iteration 6, loss = 0.57624683\n",
      "Iteration 7, loss = 0.55641391\n",
      "Iteration 8, loss = 0.55251408\n",
      "Iteration 9, loss = 0.53495373\n",
      "Iteration 10, loss = 0.52041693\n",
      "Iteration 11, loss = 0.53467058\n",
      "Iteration 12, loss = 0.59776172\n",
      "Iteration 13, loss = 0.66089106\n",
      "Iteration 14, loss = 0.56840408\n",
      "Iteration 15, loss = 0.55595354\n",
      "Iteration 16, loss = 0.56197418\n",
      "Iteration 17, loss = 0.54346026\n",
      "Iteration 18, loss = 0.55343162\n",
      "Iteration 19, loss = 0.58515156\n",
      "Iteration 20, loss = 0.52807503\n",
      "Iteration 21, loss = 0.52988612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5872672487058643"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    NeuralNetwork\n",
    "\"\"\"\n",
    "neural_network = MLPClassifier(\n",
    "    solver='adam', \n",
    "    alpha=1e-5, \n",
    "    hidden_layer_sizes=(175,125), \n",
    "    verbose=10,\n",
    "    random_state=1,\n",
    "        learning_rate_init=.1,\n",
    "    activation='tanh',\n",
    "    learning_rate='adaptive', max_iter=500\n",
    ")\n",
    "\n",
    "neural_network.fit(X_train, y_train)\n",
    "\n",
    "roc_auc_score(y_test, neural_network.predict_proba(X_test)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7426639882562002"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    AdaBooster with RandomForest\n",
    "\"\"\"\n",
    "\n",
    "adb_rf = AdaBoostClassifier( RandomForestClassifier(max_depth=100, max_features=3, min_samples_leaf=3,\n",
    "                       min_samples_split=10, n_estimators=100), random_state=0, n_estimators=100)\n",
    "adb_rf.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, adb_rf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max_features must be in (0, n_features]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-739-7808fa465258>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m ], voting='soft')\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mvoiting_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     71\u001b[0m                              % (len(self.weights), len(self.estimators)))\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[0;32m     74\u001b[0m                 delayed(_fit_single_estimator)(\n\u001b[0;32m     75\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_depth must be greater than zero. \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_features must be in (0, n_features]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n",
      "\u001b[1;31mValueError\u001b[0m: max_features must be in (0, n_features]"
     ]
    }
   ],
   "source": [
    "voiting_s = VotingClassifier(estimators=[\n",
    "    (\"AdaRandomForest\", adb_rf),\n",
    "    (\"RandomForest\", random_forest_best),\n",
    "    (\"AdaLogit\", adb_lgr),\n",
    "    (\"Gradient_Bosting\", gradient_boosting),\n",
    "    (\"KNN\", gs_knn),\n",
    "    (\"Logit\", grid_clf_acc),\n",
    "    (\"DT\", model_decision_tree),\n",
    "    (\"PCA_LG\", pca_k),\n",
    "    (\"Neural_network\", neural_network)\n",
    "], voting='soft')\n",
    "\n",
    "voiting_s.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.750181565324886"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, voiting_s.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>period</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147883</td>\n",
       "      <td>-0.338504</td>\n",
       "      <td>0.187137</td>\n",
       "      <td>-0.260786</td>\n",
       "      <td>-0.449327</td>\n",
       "      <td>-0.194205</td>\n",
       "      <td>-0.345121</td>\n",
       "      <td>-0.287067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.158198</td>\n",
       "      <td>-0.355094</td>\n",
       "      <td>-0.247809</td>\n",
       "      <td>-0.459664</td>\n",
       "      <td>0.286517</td>\n",
       "      <td>0.173683</td>\n",
       "      <td>-0.402133</td>\n",
       "      <td>-0.178256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.147883</td>\n",
       "      <td>0.251238</td>\n",
       "      <td>0.139599</td>\n",
       "      <td>-0.260786</td>\n",
       "      <td>-0.179069</td>\n",
       "      <td>-0.194205</td>\n",
       "      <td>-0.354757</td>\n",
       "      <td>-0.287067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.158198</td>\n",
       "      <td>-0.355094</td>\n",
       "      <td>0.682367</td>\n",
       "      <td>-0.504515</td>\n",
       "      <td>0.165105</td>\n",
       "      <td>0.049951</td>\n",
       "      <td>-0.142725</td>\n",
       "      <td>-0.178256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.147883</td>\n",
       "      <td>-0.338504</td>\n",
       "      <td>0.087133</td>\n",
       "      <td>-0.260786</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>-0.194205</td>\n",
       "      <td>-0.323553</td>\n",
       "      <td>-0.287067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.158198</td>\n",
       "      <td>-0.355094</td>\n",
       "      <td>-0.247809</td>\n",
       "      <td>-0.504515</td>\n",
       "      <td>0.241265</td>\n",
       "      <td>0.462394</td>\n",
       "      <td>-0.137869</td>\n",
       "      <td>-0.178256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.147883</td>\n",
       "      <td>-0.338504</td>\n",
       "      <td>-0.014464</td>\n",
       "      <td>-0.260786</td>\n",
       "      <td>-0.345622</td>\n",
       "      <td>-0.194205</td>\n",
       "      <td>-0.217303</td>\n",
       "      <td>-0.287067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.158198</td>\n",
       "      <td>-0.355094</td>\n",
       "      <td>-0.247809</td>\n",
       "      <td>-0.549366</td>\n",
       "      <td>0.696313</td>\n",
       "      <td>0.148937</td>\n",
       "      <td>-0.266513</td>\n",
       "      <td>-0.178256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.147883</td>\n",
       "      <td>-0.338504</td>\n",
       "      <td>-0.268301</td>\n",
       "      <td>-0.220220</td>\n",
       "      <td>-0.195249</td>\n",
       "      <td>-0.194205</td>\n",
       "      <td>-0.002348</td>\n",
       "      <td>-0.287067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.150839</td>\n",
       "      <td>-0.355094</td>\n",
       "      <td>-0.247809</td>\n",
       "      <td>-0.549366</td>\n",
       "      <td>0.952769</td>\n",
       "      <td>-0.321248</td>\n",
       "      <td>-0.341634</td>\n",
       "      <td>-0.178256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61267</th>\n",
       "      <td>5105</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.147883</td>\n",
       "      <td>-0.102608</td>\n",
       "      <td>-0.204894</td>\n",
       "      <td>-0.260786</td>\n",
       "      <td>-0.608160</td>\n",
       "      <td>-0.194205</td>\n",
       "      <td>-0.664722</td>\n",
       "      <td>-0.382230</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.158198</td>\n",
       "      <td>-0.355094</td>\n",
       "      <td>0.090437</td>\n",
       "      <td>-0.549366</td>\n",
       "      <td>-0.899666</td>\n",
       "      <td>-0.667701</td>\n",
       "      <td>-0.534426</td>\n",
       "      <td>-0.178256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61268</th>\n",
       "      <td>5105</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.147883</td>\n",
       "      <td>-0.102608</td>\n",
       "      <td>-0.204894</td>\n",
       "      <td>-0.260786</td>\n",
       "      <td>-0.673327</td>\n",
       "      <td>-0.194205</td>\n",
       "      <td>-0.648699</td>\n",
       "      <td>-0.097609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.158198</td>\n",
       "      <td>-0.355094</td>\n",
       "      <td>0.090437</td>\n",
       "      <td>-0.549366</td>\n",
       "      <td>-0.911767</td>\n",
       "      <td>-0.618207</td>\n",
       "      <td>-0.630876</td>\n",
       "      <td>-0.178256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61269</th>\n",
       "      <td>5105</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.147883</td>\n",
       "      <td>0.213709</td>\n",
       "      <td>-0.204894</td>\n",
       "      <td>-0.260786</td>\n",
       "      <td>-0.622293</td>\n",
       "      <td>-0.194205</td>\n",
       "      <td>-0.740273</td>\n",
       "      <td>-0.294337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.158198</td>\n",
       "      <td>-0.355094</td>\n",
       "      <td>0.496332</td>\n",
       "      <td>-0.504515</td>\n",
       "      <td>-0.827801</td>\n",
       "      <td>-0.667701</td>\n",
       "      <td>-0.559779</td>\n",
       "      <td>-0.178256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61270</th>\n",
       "      <td>5105</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.147883</td>\n",
       "      <td>-0.445730</td>\n",
       "      <td>-0.204894</td>\n",
       "      <td>-0.260786</td>\n",
       "      <td>-0.602117</td>\n",
       "      <td>-0.194205</td>\n",
       "      <td>-0.739380</td>\n",
       "      <td>-0.381878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.158198</td>\n",
       "      <td>-0.355094</td>\n",
       "      <td>-1.177986</td>\n",
       "      <td>-0.549366</td>\n",
       "      <td>-0.846816</td>\n",
       "      <td>-0.733692</td>\n",
       "      <td>-0.690409</td>\n",
       "      <td>-0.178256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61271</th>\n",
       "      <td>5105</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.147883</td>\n",
       "      <td>-0.552956</td>\n",
       "      <td>-0.204894</td>\n",
       "      <td>-0.260786</td>\n",
       "      <td>-0.734088</td>\n",
       "      <td>-0.194205</td>\n",
       "      <td>-0.737342</td>\n",
       "      <td>-0.233318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.158198</td>\n",
       "      <td>-0.355094</td>\n",
       "      <td>-1.177986</td>\n",
       "      <td>-0.549366</td>\n",
       "      <td>-0.016607</td>\n",
       "      <td>-0.733692</td>\n",
       "      <td>-0.681824</td>\n",
       "      <td>-0.178256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61272 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  period  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0         0       1  -0.147883  -0.338504   0.187137  -0.260786  -0.449327   \n",
       "1         0       2  -0.147883   0.251238   0.139599  -0.260786  -0.179069   \n",
       "2         0       3  -0.147883  -0.338504   0.087133  -0.260786  -0.102791   \n",
       "3         0       4  -0.147883  -0.338504  -0.014464  -0.260786  -0.345622   \n",
       "4         0       5  -0.147883  -0.338504  -0.268301  -0.220220  -0.195249   \n",
       "...     ...     ...        ...        ...        ...        ...        ...   \n",
       "61267  5105       8  -0.147883  -0.102608  -0.204894  -0.260786  -0.608160   \n",
       "61268  5105       9  -0.147883  -0.102608  -0.204894  -0.260786  -0.673327   \n",
       "61269  5105      10  -0.147883   0.213709  -0.204894  -0.260786  -0.622293   \n",
       "61270  5105      11  -0.147883  -0.445730  -0.204894  -0.260786  -0.602117   \n",
       "61271  5105      12  -0.147883  -0.552956  -0.204894  -0.260786  -0.734088   \n",
       "\n",
       "       feature_5  feature_6  feature_7  ...  feature_40  feature_41  \\\n",
       "0      -0.194205  -0.345121  -0.287067  ...   -0.190213         0.0   \n",
       "1      -0.194205  -0.354757  -0.287067  ...   -0.190213         0.0   \n",
       "2      -0.194205  -0.323553  -0.287067  ...   -0.184384         0.0   \n",
       "3      -0.194205  -0.217303  -0.287067  ...   -0.190213         0.0   \n",
       "4      -0.194205  -0.002348  -0.287067  ...   -0.111109         0.0   \n",
       "...          ...        ...        ...  ...         ...         ...   \n",
       "61267  -0.194205  -0.664722  -0.382230  ...   -0.190213         0.0   \n",
       "61268  -0.194205  -0.648699  -0.097609  ...   -0.044494         0.0   \n",
       "61269  -0.194205  -0.740273  -0.294337  ...    0.498412         0.0   \n",
       "61270  -0.194205  -0.739380  -0.381878  ...   -0.190213         0.0   \n",
       "61271  -0.194205  -0.737342  -0.233318  ...   -0.190213         0.0   \n",
       "\n",
       "       feature_42  feature_43  feature_44  feature_45  feature_46  feature_47  \\\n",
       "0       -0.158198   -0.355094   -0.247809   -0.459664    0.286517    0.173683   \n",
       "1       -0.158198   -0.355094    0.682367   -0.504515    0.165105    0.049951   \n",
       "2       -0.158198   -0.355094   -0.247809   -0.504515    0.241265    0.462394   \n",
       "3       -0.158198   -0.355094   -0.247809   -0.549366    0.696313    0.148937   \n",
       "4       -0.150839   -0.355094   -0.247809   -0.549366    0.952769   -0.321248   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "61267   -0.158198   -0.355094    0.090437   -0.549366   -0.899666   -0.667701   \n",
       "61268   -0.158198   -0.355094    0.090437   -0.549366   -0.911767   -0.618207   \n",
       "61269   -0.158198   -0.355094    0.496332   -0.504515   -0.827801   -0.667701   \n",
       "61270   -0.158198   -0.355094   -1.177986   -0.549366   -0.846816   -0.733692   \n",
       "61271   -0.158198   -0.355094   -1.177986   -0.549366   -0.016607   -0.733692   \n",
       "\n",
       "       feature_48  feature_49  \n",
       "0       -0.402133   -0.178256  \n",
       "1       -0.142725   -0.178256  \n",
       "2       -0.137869   -0.178256  \n",
       "3       -0.266513   -0.178256  \n",
       "4       -0.341634   -0.178256  \n",
       "...           ...         ...  \n",
       "61267   -0.534426   -0.178256  \n",
       "61268   -0.630876   -0.178256  \n",
       "61269   -0.559779   -0.178256  \n",
       "61270   -0.690409   -0.178256  \n",
       "61271   -0.681824   -0.178256  \n",
       "\n",
       "[61272 rows x 52 columns]"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.merge(replace_df(tabular_data), test, how='inner', on='id')\n",
    "test_data.drop([\"period\", \"feature_25\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_25_0</th>\n",
       "      <th>feature_25_1</th>\n",
       "      <th>feature_25_2</th>\n",
       "      <th>feature_25_3</th>\n",
       "      <th>feature_25_4</th>\n",
       "      <th>feature_25_5</th>\n",
       "      <th>feature_25_6</th>\n",
       "      <th>feature_25_7</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4084</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-0.446827</td>\n",
       "      <td>0.223741</td>\n",
       "      <td>-1.937912</td>\n",
       "      <td>-2.877515</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>-6.159996</td>\n",
       "      <td>1.104288</td>\n",
       "      <td>-0.252435</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4085</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-1.263459</td>\n",
       "      <td>-2.306513</td>\n",
       "      <td>-3.120488</td>\n",
       "      <td>-4.649825</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>5.615267</td>\n",
       "      <td>-4.646977</td>\n",
       "      <td>-3.193513</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4086</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-2.635949</td>\n",
       "      <td>-0.778186</td>\n",
       "      <td>-3.129430</td>\n",
       "      <td>19.607365</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>-9.099560</td>\n",
       "      <td>40.032339</td>\n",
       "      <td>16.477153</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4087</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-6.544331</td>\n",
       "      <td>-1.184435</td>\n",
       "      <td>-1.758981</td>\n",
       "      <td>-7.188640</td>\n",
       "      <td>-0.315561</td>\n",
       "      <td>9.267171</td>\n",
       "      <td>1.334315</td>\n",
       "      <td>-3.496790</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4088</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>4.810456</td>\n",
       "      <td>-3.547540</td>\n",
       "      <td>-0.101132</td>\n",
       "      <td>-4.249702</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>1.315559</td>\n",
       "      <td>-1.256908</td>\n",
       "      <td>4.014597</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>215</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>5101</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>0.977561</td>\n",
       "      <td>-0.064880</td>\n",
       "      <td>-2.932252</td>\n",
       "      <td>18.973592</td>\n",
       "      <td>-2.166317</td>\n",
       "      <td>-1.771401</td>\n",
       "      <td>29.433597</td>\n",
       "      <td>1.609543</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>5102</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-0.758425</td>\n",
       "      <td>7.598306</td>\n",
       "      <td>-3.129435</td>\n",
       "      <td>-4.486197</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>3.366962</td>\n",
       "      <td>-6.755492</td>\n",
       "      <td>-2.022724</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>5103</td>\n",
       "      <td>4.821675</td>\n",
       "      <td>-8.235068</td>\n",
       "      <td>8.355527</td>\n",
       "      <td>-3.129435</td>\n",
       "      <td>-6.129909</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>4.679350</td>\n",
       "      <td>-9.026192</td>\n",
       "      <td>-8.864082</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>5104</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-0.043550</td>\n",
       "      <td>-3.763495</td>\n",
       "      <td>-2.046995</td>\n",
       "      <td>-7.827949</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>-0.384948</td>\n",
       "      <td>-8.755244</td>\n",
       "      <td>-4.963803</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>5105</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-1.300988</td>\n",
       "      <td>-2.458731</td>\n",
       "      <td>-3.129435</td>\n",
       "      <td>-7.565001</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>-8.144803</td>\n",
       "      <td>-3.709773</td>\n",
       "      <td>-7.622764</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1022 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0     4084  -1.774593  -0.446827   0.223741  -1.937912  -2.877515  -2.330461   \n",
       "1     4085  -1.774593  -1.263459  -2.306513  -3.120488  -4.649825  -2.330461   \n",
       "2     4086  -1.774593  -2.635949  -0.778186  -3.129430  19.607365  -2.330461   \n",
       "3     4087  -1.774593  -6.544331  -1.184435  -1.758981  -7.188640  -0.315561   \n",
       "4     4088  -1.774593   4.810456  -3.547540  -0.101132  -4.249702  -2.330461   \n",
       "...    ...        ...        ...        ...        ...        ...        ...   \n",
       "1017  5101  -1.774593   0.977561  -0.064880  -2.932252  18.973592  -2.166317   \n",
       "1018  5102  -1.774593  -0.758425   7.598306  -3.129435  -4.486197  -2.330461   \n",
       "1019  5103   4.821675  -8.235068   8.355527  -3.129435  -6.129909  -2.330461   \n",
       "1020  5104  -1.774593  -0.043550  -3.763495  -2.046995  -7.827949  -2.330461   \n",
       "1021  5105  -1.774593  -1.300988  -2.458731  -3.129435  -7.565001  -2.330461   \n",
       "\n",
       "      feature_6  feature_7  feature_8  ...  feature_25_0  feature_25_1  \\\n",
       "0     -6.159996   1.104288  -0.252435  ...             0             0   \n",
       "1      5.615267  -4.646977  -3.193513  ...             9             0   \n",
       "2     -9.099560  40.032339  16.477153  ...             0             0   \n",
       "3      9.267171   1.334315  -3.496790  ...             0             0   \n",
       "4      1.315559  -1.256908   4.014597  ...             0             0   \n",
       "...         ...        ...        ...  ...           ...           ...   \n",
       "1017  -1.771401  29.433597   1.609543  ...             0             0   \n",
       "1018   3.366962  -6.755492  -2.022724  ...             0             0   \n",
       "1019   4.679350  -9.026192  -8.864082  ...             0             0   \n",
       "1020  -0.384948  -8.755244  -4.963803  ...             0             0   \n",
       "1021  -8.144803  -3.709773  -7.622764  ...             0             0   \n",
       "\n",
       "      feature_25_2  feature_25_3  feature_25_4  feature_25_5  feature_25_6  \\\n",
       "0                0             1             0             0             0   \n",
       "1                0             3             0             0             0   \n",
       "2                0            12             0             0             0   \n",
       "3                0             0             0             0             0   \n",
       "4                0             0             0             0             0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1017             0             0             0             0             0   \n",
       "1018             0            12             0             0             0   \n",
       "1019            12             0             0             0             0   \n",
       "1020             0             0             0             0             0   \n",
       "1021             0            12             0             0             0   \n",
       "\n",
       "      feature_25_7  feature_50  score  \n",
       "0               11          11    NaN  \n",
       "1                0           1    NaN  \n",
       "2                0          19    NaN  \n",
       "3               12          53    NaN  \n",
       "4               12         215    NaN  \n",
       "...            ...         ...    ...  \n",
       "1017            12         107    NaN  \n",
       "1018             0          83    NaN  \n",
       "1019             0           6    NaN  \n",
       "1020            12          10    NaN  \n",
       "1021             0           3    NaN  \n",
       "\n",
       "[1022 rows x 60 columns]"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = union_with_target(\n",
    "                        replace_df(tabular_data),\n",
    "                        OneHotEncoder_features(tabular_data, \"feature_25\"),\n",
    "                        OneHotEncoder_for_Hashed_data(hashed_feature),\n",
    "                        test\n",
    ")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop(['score'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(test_data[[f'feature_25_{i}' for i in range(0,8)]+[\"feature_50\"]])\n",
    "\n",
    "test_data[[f'feature_25_{i}' for i in range(0,8)]+[\"feature_50\"]] = sc.fit_transform(test_data[[f'feature_25_{i}' for i in range(0,8)]+[\"feature_50\"]])[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_25_0</th>\n",
       "      <th>feature_25_1</th>\n",
       "      <th>feature_25_2</th>\n",
       "      <th>feature_25_3</th>\n",
       "      <th>feature_25_4</th>\n",
       "      <th>feature_25_5</th>\n",
       "      <th>feature_25_6</th>\n",
       "      <th>feature_25_7</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4085</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-1.263459</td>\n",
       "      <td>-2.306513</td>\n",
       "      <td>-3.120488</td>\n",
       "      <td>-4.649825</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>5.615267</td>\n",
       "      <td>-4.646977</td>\n",
       "      <td>-3.193513</td>\n",
       "      <td>...</td>\n",
       "      <td>3.045661</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>-0.331043</td>\n",
       "      <td>0.080540</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>-1.315129</td>\n",
       "      <td>-0.602874</td>\n",
       "      <td>0.258817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4086</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-2.635949</td>\n",
       "      <td>-0.778186</td>\n",
       "      <td>-3.129430</td>\n",
       "      <td>19.607365</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>-9.099560</td>\n",
       "      <td>40.032339</td>\n",
       "      <td>16.477153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333590</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>-0.331043</td>\n",
       "      <td>2.050769</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>-1.315129</td>\n",
       "      <td>-0.403831</td>\n",
       "      <td>0.198353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4087</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-6.544331</td>\n",
       "      <td>-1.184435</td>\n",
       "      <td>-1.758981</td>\n",
       "      <td>-7.188640</td>\n",
       "      <td>-0.315561</td>\n",
       "      <td>9.267171</td>\n",
       "      <td>1.334315</td>\n",
       "      <td>-3.496790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333590</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>-0.331043</td>\n",
       "      <td>-0.576203</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>0.898837</td>\n",
       "      <td>-0.027861</td>\n",
       "      <td>0.242178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4088</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>4.810456</td>\n",
       "      <td>-3.547540</td>\n",
       "      <td>-0.101132</td>\n",
       "      <td>-4.249702</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>1.315559</td>\n",
       "      <td>-1.256908</td>\n",
       "      <td>4.014597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333590</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>-0.331043</td>\n",
       "      <td>-0.576203</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>0.898837</td>\n",
       "      <td>1.763526</td>\n",
       "      <td>0.328983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4089</td>\n",
       "      <td>4.821675</td>\n",
       "      <td>-11.105825</td>\n",
       "      <td>0.526756</td>\n",
       "      <td>-3.129435</td>\n",
       "      <td>-9.517217</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>-1.604352</td>\n",
       "      <td>-5.851987</td>\n",
       "      <td>-8.589017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333590</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>-0.331043</td>\n",
       "      <td>-0.576203</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>0.898837</td>\n",
       "      <td>-0.558643</td>\n",
       "      <td>0.211198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>5101</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>0.977561</td>\n",
       "      <td>-0.064880</td>\n",
       "      <td>-2.932252</td>\n",
       "      <td>18.973592</td>\n",
       "      <td>-2.166317</td>\n",
       "      <td>-1.771401</td>\n",
       "      <td>29.433597</td>\n",
       "      <td>1.609543</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333590</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>-0.331043</td>\n",
       "      <td>-0.576203</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>0.898837</td>\n",
       "      <td>0.569268</td>\n",
       "      <td>0.194275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>5102</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-0.758425</td>\n",
       "      <td>7.598306</td>\n",
       "      <td>-3.129435</td>\n",
       "      <td>-4.486197</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>3.366962</td>\n",
       "      <td>-6.755492</td>\n",
       "      <td>-2.022724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333590</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>-0.331043</td>\n",
       "      <td>2.050769</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>-1.315129</td>\n",
       "      <td>0.303877</td>\n",
       "      <td>0.371863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>5103</td>\n",
       "      <td>4.821675</td>\n",
       "      <td>-8.235068</td>\n",
       "      <td>8.355527</td>\n",
       "      <td>-3.129435</td>\n",
       "      <td>-6.129909</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>4.679350</td>\n",
       "      <td>-9.026192</td>\n",
       "      <td>-8.864082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333590</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>3.557762</td>\n",
       "      <td>-0.576203</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>-1.315129</td>\n",
       "      <td>-0.547585</td>\n",
       "      <td>0.227673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>5104</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-0.043550</td>\n",
       "      <td>-3.763495</td>\n",
       "      <td>-2.046995</td>\n",
       "      <td>-7.827949</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>-0.384948</td>\n",
       "      <td>-8.755244</td>\n",
       "      <td>-4.963803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333590</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>-0.331043</td>\n",
       "      <td>-0.576203</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>0.898837</td>\n",
       "      <td>-0.503353</td>\n",
       "      <td>0.325185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>5105</td>\n",
       "      <td>-1.774593</td>\n",
       "      <td>-1.300988</td>\n",
       "      <td>-2.458731</td>\n",
       "      <td>-3.129435</td>\n",
       "      <td>-7.565001</td>\n",
       "      <td>-2.330461</td>\n",
       "      <td>-8.144803</td>\n",
       "      <td>-3.709773</td>\n",
       "      <td>-7.622764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333590</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>-0.331043</td>\n",
       "      <td>2.050769</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>-0.031296</td>\n",
       "      <td>-0.074942</td>\n",
       "      <td>-1.315129</td>\n",
       "      <td>-0.580759</td>\n",
       "      <td>0.119886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1021 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "1     4085  -1.774593  -1.263459  -2.306513  -3.120488  -4.649825  -2.330461   \n",
       "2     4086  -1.774593  -2.635949  -0.778186  -3.129430  19.607365  -2.330461   \n",
       "3     4087  -1.774593  -6.544331  -1.184435  -1.758981  -7.188640  -0.315561   \n",
       "4     4088  -1.774593   4.810456  -3.547540  -0.101132  -4.249702  -2.330461   \n",
       "5     4089   4.821675 -11.105825   0.526756  -3.129435  -9.517217  -2.330461   \n",
       "...    ...        ...        ...        ...        ...        ...        ...   \n",
       "1017  5101  -1.774593   0.977561  -0.064880  -2.932252  18.973592  -2.166317   \n",
       "1018  5102  -1.774593  -0.758425   7.598306  -3.129435  -4.486197  -2.330461   \n",
       "1019  5103   4.821675  -8.235068   8.355527  -3.129435  -6.129909  -2.330461   \n",
       "1020  5104  -1.774593  -0.043550  -3.763495  -2.046995  -7.827949  -2.330461   \n",
       "1021  5105  -1.774593  -1.300988  -2.458731  -3.129435  -7.565001  -2.330461   \n",
       "\n",
       "      feature_6  feature_7  feature_8  ...  feature_25_0  feature_25_1  \\\n",
       "1      5.615267  -4.646977  -3.193513  ...      3.045661     -0.177084   \n",
       "2     -9.099560  40.032339  16.477153  ...     -0.333590     -0.177084   \n",
       "3      9.267171   1.334315  -3.496790  ...     -0.333590     -0.177084   \n",
       "4      1.315559  -1.256908   4.014597  ...     -0.333590     -0.177084   \n",
       "5     -1.604352  -5.851987  -8.589017  ...     -0.333590     -0.177084   \n",
       "...         ...        ...        ...  ...           ...           ...   \n",
       "1017  -1.771401  29.433597   1.609543  ...     -0.333590     -0.177084   \n",
       "1018   3.366962  -6.755492  -2.022724  ...     -0.333590     -0.177084   \n",
       "1019   4.679350  -9.026192  -8.864082  ...     -0.333590     -0.177084   \n",
       "1020  -0.384948  -8.755244  -4.963803  ...     -0.333590     -0.177084   \n",
       "1021  -8.144803  -3.709773  -7.622764  ...     -0.333590     -0.177084   \n",
       "\n",
       "      feature_25_2  feature_25_3  feature_25_4  feature_25_5  feature_25_6  \\\n",
       "1        -0.331043      0.080540     -0.059218     -0.031296     -0.074942   \n",
       "2        -0.331043      2.050769     -0.059218     -0.031296     -0.074942   \n",
       "3        -0.331043     -0.576203     -0.059218     -0.031296     -0.074942   \n",
       "4        -0.331043     -0.576203     -0.059218     -0.031296     -0.074942   \n",
       "5        -0.331043     -0.576203     -0.059218     -0.031296     -0.074942   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1017     -0.331043     -0.576203     -0.059218     -0.031296     -0.074942   \n",
       "1018     -0.331043      2.050769     -0.059218     -0.031296     -0.074942   \n",
       "1019      3.557762     -0.576203     -0.059218     -0.031296     -0.074942   \n",
       "1020     -0.331043     -0.576203     -0.059218     -0.031296     -0.074942   \n",
       "1021     -0.331043      2.050769     -0.059218     -0.031296     -0.074942   \n",
       "\n",
       "      feature_25_7  feature_50     score  \n",
       "1        -1.315129   -0.602874  0.258817  \n",
       "2        -1.315129   -0.403831  0.198353  \n",
       "3         0.898837   -0.027861  0.242178  \n",
       "4         0.898837    1.763526  0.328983  \n",
       "5         0.898837   -0.558643  0.211198  \n",
       "...            ...         ...       ...  \n",
       "1017      0.898837    0.569268  0.194275  \n",
       "1018     -1.315129    0.303877  0.371863  \n",
       "1019     -1.315129   -0.547585  0.227673  \n",
       "1020      0.898837   -0.503353  0.325185  \n",
       "1021     -1.315129   -0.580759  0.119886  \n",
       "\n",
       "[1021 rows x 60 columns]"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"score\"] = voiting_s.predict_proba(test_data[test_data.columns[1:-1]])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x282aa6f6c70>"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data[\"score\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"TrotsenkoDaniil_test.csv\",index=None,columns=['id','score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit64566c359c824f83abd7bf1ca62dd74d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
